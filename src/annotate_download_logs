#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Take postprocessed logs and strip out multiple hits in sessions, and
resolve URLs to the chosen `URI_SCHEME` (e.g. info:doi).

Logs come in as a CSV of 4-tuples of type
 (timestamp * IP address * URL * user agent)

We strip out entries where the same (IP address * user agent) pair has accessed
a URL within the last `SESSION_TIMEOUT` (e.g. half-hour)

Additionally, we convert the URLs to ISBNs and collate request data by date,
outputting a CSV for ingest via the stats system.
"""

import os
import csv
import sys
import json
import time
import httplib2
import datetime
from optparse import OptionParser


SESSION_TIMEOUT = int(os.environ['SESSION_TIMEOUT'])
ROLLOVER        = os.environ['ROLLOVER'] in ('True', 'true', 't', 1)
URI_API_ENDP    = os.environ['URI_API_ENDP']
URI_API_USER    = os.environ['URI_API_USER']
URI_API_PASS    = os.environ['URI_API_PASS']
AUTH_API_ENDP   = os.environ['AUTH_API_ENDP']
URI_SCHEME      = os.environ['URI_SCHEME']
URI_STRICT      = os.environ['URI_STRICT']
EXCLUDED_URLS   = json.loads(os.getenv('EXCLUDED_URLS'))
CACHE           = {}
ARGS = [
    {
        'val': '--measure',
        'dest': 'measure',
        'action': 'store',
        'default': None,
        'help': 'Measure URI, e.g. tag:operas.eu,2018:readership:obp-pdf'
    }, {
        'val': '--add-headers',
        'dest': 'add_headers',
        'action': 'store_true',
        'default': [],
        'help': 'Whether to print report headers'
    }
]


def get_token(url, email, passwd):
    h = httplib2.Http()
    credentials = {'email': email, 'password': passwd}
    headers = {'content-type': 'application/json'}
    res, content = h.request(url, 'POST', json.dumps(credentials), headers)
    try:
        assert res.status == 200
    except AssertionError:
        raise ValueError(content)
    return json.loads(content)['data'][0]['token']


def get_options(args):
    parser = OptionParser()
    for arg in args:
        parser.add_option(arg['val'], dest=arg['dest'], default=arg['default'],
                          action=arg['action'], help=arg['help'])
    options, rest = parser.parse_args()

    assert rest == []
    assert options.measure
    return options


def url_to_id(url, timestamp):
    if url in CACHE:
        return CACHE[url]
    req = "%s?uri=%s&filter=uri_scheme:%s&strict=%s" \
          % (URI_API_ENDP, url, URI_SCHEME, URI_STRICT)
    h = httplib2.Http()
    res, content = h.request(req, 'GET', headers={'Authorization': AUTH})
    try:
        assert res.status == 200
    except AssertionError:
        if url in EXCLUDED_URLS:
            return []
        r = json.loads(content)
        p = (r['message'], r['parameters']['uri'], timestamp)
        print("%s: %s (%s)" % p, file=sys.stderr)
        sys.exit(1)
    entry = json.loads(content)['data']
    CACHE[url] = entry
    return entry


def resolve(url_to_id):
    """
    Read in CSV data from stdin; lazily return a stream of tuples of
    type: (timestamp * ip_address * uri * str), where the final <str>
    is the user-agent string used by the browser; each tuple represents
    an HTTP request.

    `get_node` is a callback of type (url -> timestamp -> node | None)
    """
    r = csv.reader(sys.stdin)

    for timestamp, ip_address, url, agent in r:
        identifiers = url_to_id(url, timestamp)

        excluded = identifiers == []
        if excluded:
            continue

        ts = time.strptime(timestamp, '%Y-%m-%d %H:%M:%S')
        ds = datetime.datetime(*ts[:6])

        yield (ds, ip_address, identifiers, agent)


def strip_sessions(get_node, session_timeout, rollover):
    """
    Take a lazy stream whose items are of type:
     (timestamp * ip_address * uri * str)
    and emit a lazy stream of items of type:
     (date * ip_address, isbn)

    Filter out some of the items, if some of them form part of the same session

    See elsewhere for the semantics of the argument `get_node`

    The switch `rollover` affects the behaviour of the machinery for
    determining which requests form part of the same session; if False,
    a session is no longer than `session_timeout`. If True, a session
    may be longer than `session_timeout`, so long as `session_timeout`
    has not elapsed between any two requests.
    """

    session = {}
    for timestamp, ip_address, identifiers, agent in resolve(url_to_id):
        for identifier in identifiers:
            uri = identifier['URI']
            browser = (ip_address, agent, uri)
            if browser not in session:
                session[browser] = timestamp
            else:
                last = session[browser]
                if rollover:
                    session[browser] = timestamp

                offset = (timestamp - last).seconds
                if offset < session_timeout:
                    continue

                if not rollover:
                    session[browser] = timestamp

            date = datetime.datetime(timestamp.year, timestamp.month,
                                     timestamp.day)
            yield date, ip_address, uri


def run(measure, add_headers):
    hits = {}
    for date, ip_address, uri in strip_sessions(
            url_to_id, SESSION_TIMEOUT, ROLLOVER):
        key = (date, uri)
        if key not in hits:
            hits[key] = 0
        hits[key] += 1

    w = csv.writer(sys.stdout)
    if add_headers:
        w.writerow(('measure_id', 'timestamp', 'work_uri',
                    'country_uri', 'event_uri', 'value'))

    for key, value in hits.items():
        row = tuple([measure] + list(key) + [''] + [''] + [value])
        w.writerow(row)


API_JWTOKEN = get_token(AUTH_API_ENDP, URI_API_USER, URI_API_PASS)
AUTH = 'Bearer ' + API_JWTOKEN

if __name__ == '__main__':
    options = get_options(ARGS)
    run(options.measure, options.add_headers)
